{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b843558-107e-4a35-95d3-788c27809f8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/enron1/ham'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m full_data\n\u001b[1;32m     29\u001b[0m     positive_samples \u001b[38;5;241m=\u001b[39m load_files(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/enron1/spam\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m negative_samples \u001b[38;5;241m=\u001b[39m load_files(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/enron1/ham\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[24], line 25\u001b[0m, in \u001b[0;36mload_files\u001b[0;34m(dir)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_files\u001b[39m(\u001b[38;5;28mdir\u001b[39m):\n\u001b[1;32m     24\u001b[0m     full_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;28mdir\u001b[39m):\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mdir\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m file_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mISO-8859-1\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     27\u001b[0m             full_data\u001b[38;5;241m.\u001b[39mappend(f\u001b[38;5;241m.\u001b[39mread())\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/enron1/ham'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import collections\n",
    "import nltk\n",
    "from nltk.classify import NaiveBayesClassifier, accuracy\n",
    "# nltk.download('punkt')\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "stop_words = {\n",
    "    'ourselves', 'hers', 'between', 'yourself', 'but', 'again', \n",
    "    'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they',\n",
    "    'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', \n",
    "    'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as',\n",
    "    'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we',\n",
    "    'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more',\n",
    "    'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above',\n",
    "    'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any',\n",
    "    'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does',\n",
    "    'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can',\n",
    "    'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where',\n",
    "    'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't',\n",
    "    'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how',\n",
    "    'further', 'was', 'here', 'than'}\n",
    "def load_files(dir):\n",
    "    full_data = []\n",
    "    for file_name in os.listdir(dir):\n",
    "        with open(dir + '/' + file_name, 'r', encoding='ISO-8859-1') as f:\n",
    "            full_data.append(f.read())\n",
    "    return full_data\n",
    "    positive_samples = load_files('./data/enron1/spam')\n",
    "negative_samples = load_files('./data/enron1/ham')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f67acd08-27b2-42e1-852b-7171ee401cb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'positive_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m positive_samples[:\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'positive_samples' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "positive_samples[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc5566d2-11eb-47d9-b76b-c9edd0eb4eb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2232720017.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[30], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    negative_samples[:1]def preprocess_sentence(sentence):\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "negative_samples[:1]def preprocess_sentence(sentence):\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # find least common elements\n",
    "    word_counts = collections.Counter(tokens)\n",
    "    uncommon_words = word_counts.most_common()[:-10:-1]\n",
    "    # filter tokens based on the following\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    tokens = [w for w in tokens if w not in uncommon_words]\n",
    "    #lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens] \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a06c734e-c55a-4609-b8c4-fa8c9ffaa89c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'positive_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Let us have a look at an email\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m email \u001b[38;5;129;01min\u001b[39;00m positive_samples[:\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(email)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'positive_samples' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Let us have a look at an email\n",
    "for email in positive_samples[:1]:\n",
    "    print(email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc3bf200-7b82-4f7d-96a1-50741fc47098",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'positive_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m positive_samples \u001b[38;5;241m=\u001b[39m [preprocess_sentence(email) \u001b[38;5;28;01mfor\u001b[39;00m email \u001b[38;5;129;01min\u001b[39;00m positive_samples]\n\u001b[1;32m      2\u001b[0m negative_samples \u001b[38;5;241m=\u001b[39m [preprocess_sentence(email) \u001b[38;5;28;01mfor\u001b[39;00m email \u001b[38;5;129;01min\u001b[39;00m negative_samples]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'positive_samples' is not defined"
     ]
    }
   ],
   "source": [
    "positive_samples = [preprocess_sentence(email) for email in positive_samples]\n",
    "negative_samples = [preprocess_sentence(email) for email in negative_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d4c22ab2-ffc6-4644-b80c-0c4bd415ef6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'positive_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m positive_samples \u001b[38;5;241m=\u001b[39m [(email, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m email \u001b[38;5;129;01min\u001b[39;00m positive_samples]\n\u001b[1;32m      2\u001b[0m negative_samples \u001b[38;5;241m=\u001b[39m [(email, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m email \u001b[38;5;129;01min\u001b[39;00m negative_samples]\n\u001b[1;32m      3\u001b[0m all_samples \u001b[38;5;241m=\u001b[39m positive_samples \u001b[38;5;241m+\u001b[39m negative_samples\n",
      "\u001b[0;31mNameError\u001b[0m: name 'positive_samples' is not defined"
     ]
    }
   ],
   "source": [
    "positive_samples = [(email, 1) for email in positive_samples]\n",
    "negative_samples = [(email, 0) for email in negative_samples]\n",
    "all_samples = positive_samples + negative_samples\n",
    "random.shuffle(all_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "75356162-4161-44dd-bdaf-ef9ac2f12113",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_samples)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m emails processed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_samples' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"{len(all_samples)} emails processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e1b9fb7-7b85-42ac-95a8-0bf3f4772890",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Feature extraction\n",
    "def feature_extraction(tokens):\n",
    "    # Each word will be a feature and feature value will be word count\n",
    "    return dict(collections.Counter(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c8295cab-7e83-4b16-994c-cb595323db16",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# features = [(feature_extraction(corpus), label) for corpus, label in all_samples]\u001b[39;00m\n\u001b[1;32m      2\u001b[0m features \u001b[38;5;241m=\u001b[39m [(feature_extraction(corpus), label)\n\u001b[0;32m----> 3\u001b[0m               \u001b[38;5;28;01mfor\u001b[39;00m corpus, label \u001b[38;5;129;01min\u001b[39;00m all_samples]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_samples' is not defined"
     ]
    }
   ],
   "source": [
    "# features = [(feature_extraction(corpus), label) for corpus, label in all_samples]\n",
    "features = [(feature_extraction(corpus), label)\n",
    "              for corpus, label in all_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "206d8673-09f9-4297-9347-d53cd702cf8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m features[:\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "features[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "95a17579-b1b9-4bdf-bd54-5e76ef46441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "def train_test_split(dataset, train_size=0.8):\n",
    "    num_train_samples = int(len(dataset) * train_size)\n",
    "    return dataset[:num_train_samples], dataset[num_train_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "066d36f7-dadc-42d2-9934-7c368dcb1621",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m training_set, test_set \u001b[38;5;241m=\u001b[39m train_test_split(features, train_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "training_set, test_set = train_test_split(features, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9df9423b-4501-4b0c-abda-b4d79703cfd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mclassify\u001b[38;5;241m.\u001b[39mNaiveBayesClassifier\u001b[38;5;241m.\u001b[39mtrain(training_set)\n\u001b[1;32m      2\u001b[0m training_error \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mclassify\u001b[38;5;241m.\u001b[39maccuracy(model, training_set)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel training complete. Accuracy on training set: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_error\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_set' is not defined"
     ]
    }
   ],
   "source": [
    "model = nltk.classify.NaiveBayesClassifier.train(training_set)\n",
    "training_error = nltk.classify.accuracy(model, training_set)\n",
    "print(f'Model training complete. Accuracy on training set: {training_error}')\n",
    "\n",
    "testing_error = nltk.classify.accuracy(model, test_set)\n",
    "print(f'Accuracy on test set: {testing_error}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
